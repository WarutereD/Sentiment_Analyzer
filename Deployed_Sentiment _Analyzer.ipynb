{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We First feed raw data to the model which cleans the data by;\n",
    "1. Cleaning \n",
    "2. preparing the text data \n",
    "3. removing stopwords, punctuation, and other noise\n",
    "The data is then saved as a pdf, which contains clean data without any sentiment label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.mwananchi.co.tz/'  # Replace with the URL of the website you want to scrape\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find all the <p> tags on the page\n",
    "paragraphs = soup.find_all('p')\n",
    "\n",
    "# Print the text of each paragraph\n",
    "for p in paragraphs:\n",
    "    print(p.text)\n",
    "    print(\"hello\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pd.read_csv('Sentiment_Analyzer/dataset/Dataset_1.csv')\n",
    "\n",
    "# Remove URLs\n",
    "data['text'] = data['text'].apply(lambda x: re.sub(r'http\\S+', '', x))\n",
    "\n",
    "# Remove special characters\n",
    "data['text'] = data['text'].apply(lambda x: re.sub(r'[^a-zA-Z0-9]+', ' ', x))\n",
    "\n",
    "# Convert text to lowercase\n",
    "data['text'] = data['text'].apply(lambda x: x.lower())\n",
    "\n",
    "# Tokenize the text data\n",
    "data['tokenized_text'] = data['text'].apply(lambda x: word_tokenize(x))\n",
    "\n",
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('swahili'))\n",
    "\n",
    "data['filtered_tokens'] = data['tokenized_text'].apply(lambda x: [token for token in x if token not in stop_words])\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "data['stemmed_tokens'] = data['filtered_tokens'].apply(lambda x: [stemmer.stem(token) for token in x])\n",
    "\n",
    "# Display a sample of the stemmed data\n",
    "print(data['stemmed_tokens'].sample(min(10, len(data)), replace=True))\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "data['lemmatized_tokens'] = data['filtered_tokens'].apply(lambda x: [lemmatizer.lemmatize(token) for token in x])\n",
    "\n",
    "# Display a sample of the lemmatized data\n",
    "print(data['lemmatized_tokens'].sample(min(10, len(data)), replace=True))\n",
    "\n",
    "all_words = ' '.join([word for tokens in data['lemmatized_tokens'] for word in tokens])\n",
    "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110, stopwords=STOPWORDS).generate(all_words)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "data.to_csv('Sentiment_Analyzer/dataset/Cleaned_Dataset_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from textblob import TextBlob\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of       Unnamed: 0                                             maneno     lugha\n",
       "0              0                 team 2019merimera alikuwa takataka  negative\n",
       "1              1                                     sijafurahishwa  negative\n",
       "2              2                                      kubuni dosari  negative\n",
       "3              3                  bila kusema nilipoteza pesa zangu  negative\n",
       "4              4                       sema kupoteza pesa na wakati  negative\n",
       "...          ...                                                ...       ...\n",
       "3920        2995  Nafikiri chakula chapasa kuwa na ladha na umbi...  negative\n",
       "3921        2996                   hamu ya kula ilitoweka mara moja  negative\n",
       "3922        2997            Kwa ujumla sikuvutiwa na nisirudi nyuma  negative\n",
       "3923        2998  Mambo yote yaliyoonwa yalikuwa chini ya kiwang...  negative\n",
       "3924        2999  Basi ni kana kwamba nilipoteza maisha yangu ya...  negative\n",
       "\n",
       "[3925 rows x 3 columns]>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(\"/workspaces/Sentiment_Analyzer/Sentiment_Analyzer/dataset/swahili.csv\")\n",
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing\n",
    "stop_words = stopwords.words(\"swahili\")\n",
    "df[\"maneno\"] = df[\"maneno\"].apply(lambda x: \" \".join([word for word in re.sub('[^a-zA-Z0-9\\s]', '', x).split() if word not in stop_words]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"maneno\"], df[\"lugha\"], test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the text data using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train an SVM classifier\n",
    "svm = SVC(kernel='linear', C=1.0)\n",
    "svm.fit(X_train_vec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for swahili.csv\n",
      "Accuracy: 0.78\n",
      "F1 Score: 0.78\n",
      "Polarity: -0.09\n"
     ]
    }
   ],
   "source": [
    "# Make predictions and print results\n",
    "y_pred = svm.predict(X_test_vec)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f\"Results for swahili.csv\")\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"F1 Score: {f1_score:.2f}\")\n",
    "print(f\"Polarity: {TextBlob(' '.join(df['maneno'])).sentiment.polarity:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text):\n",
    "    # Preprocess the input text\n",
    "    text = \" \".join([word for word in re.sub('[^a-zA-Z0-9\\s]', '', text).split() if word not in stop_words])\n",
    "    # Vectorize the input text\n",
    "    text_vec = vectorizer.transform([text])\n",
    "    # Predict the sentiment of the input text\n",
    "    sentiment = svm.predict(text_vec)[0]\n",
    "    return sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentimentcsv(data):\n",
    "    # Preprocess the new data\n",
    "    data[\"maneno\"] = data[\"maneno\"].apply(lambda x: \" \".join([word for word in re.sub('[^a-zA-Z0-9\\s]', '', x).split() if word not in stop_words]))\n",
    "\n",
    "    # Vectorize the new data\n",
    "    new_data_vec = vectorizer.transform(data[\"maneno\"])\n",
    "\n",
    "    # Predict the sentiment of the new data\n",
    "    sentiment = svm.predict(new_data_vec)\n",
    "\n",
    "    # Calculate the polarity scores of the new data\n",
    "    new_data_polarity = [TextBlob(text).sentiment.polarity for text in data[\"maneno\"]]\n",
    "\n",
    "    # Calculate the precision, recall, F1 score, and support for the new data\n",
    "    precision_new, recall_new, f1_score_new, support_new = precision_recall_fscore_support(sentiment, data[\"lugha\"], average='weighted')\n",
    "\n",
    "    # Calculate the accuracy for the new data\n",
    "    accuracy_new = accuracy_score(data[\"lugha\"], sentiment)*100\n",
    "\n",
    "    return sentiment, new_data_polarity, precision_new, recall_new, f1_score_new, support_new, accuracy_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Gradio interface\n",
    "#input_text = gr.inputs.Textbox(label=\"Input Text\")\n",
    "#output_sentiment = gr.outputs.Label(label=\"Sentiment Prediction\")\n",
    "#gr.Interface(fn=predict_sentiment, inputs=input_text, outputs=output_sentiment, \n",
    "  #           title=\"Swahili Sentiment Analyzer\", description=\"Predict the sentiment of Swahili text using an SVM classifier trained on a dataset of Swahili text.\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def load_csv(file_path):\n",
    "    # Load the CSV data\n",
    " #   df = pd.read_csv(file_path)\n",
    "\n",
    "    # Text preprocessing\n",
    "  #  df[\"maneno\"] = df[\"maneno\"].apply(lambda x: \" \".join([word for word in re.sub('[^a-zA-Z0-9\\s]', '', x).split() if word not in stop_words]))\n",
    "\n",
    "   # return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def get_sentiment_metrics(df):\n",
    "    # Vectorize the text data using TF-IDF\n",
    " #   X_vec = vectorizer.transform(df[\"maneno\"])\n",
    "\n",
    "    # Make predictions and print results\n",
    "  #  y_pred = svm.predict(X_vec)\n",
    "   # accuracy = accuracy_score(df[\"lugha\"], y_pred)\n",
    "    #precision, recall, f1_score, _ = precision_recall_fscore_support(df[\"lugha\"], y_pred, average='weighted')\n",
    "    \n",
    "    #print(f\"Results for input CSV data\")\n",
    "    #print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    #print(f\"F1 Score: {f1_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/gradio/inputs.py:347: UserWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your components from gradio.components\n",
      "  warnings.warn(\n",
      "/home/codespace/.python/current/lib/python3.10/site-packages/gradio/deprecation.py:40: UserWarning: `optional` parameter is deprecated, and it has no effect\n",
      "  warnings.warn(value)\n",
      "/home/codespace/.python/current/lib/python3.10/site-packages/gradio/deprecation.py:40: UserWarning: `keep_filename` parameter is deprecated, and it has no effect\n",
      "  warnings.warn(value)\n",
      "/home/codespace/.python/current/lib/python3.10/site-packages/gradio/inputs.py:27: UserWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
      "  warnings.warn(\n",
      "/home/codespace/.python/current/lib/python3.10/site-packages/gradio/deprecation.py:40: UserWarning: `numeric` parameter is deprecated, and it has no effect\n",
      "  warnings.warn(value)\n",
      "/home/codespace/.python/current/lib/python3.10/site-packages/gradio/outputs.py:22: UserWarning: Usage of gradio.outputs is deprecated, and will not be supported in the future, please import your components from gradio.components\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7862\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import pandas as pd\n",
    "#from swahili_sentiment import test_sentiment, predict_sentiment\n",
    "\n",
    "# Define input and output interfaces\n",
    "csv_input = gr.inputs.File(label=\"Upload CSV file\")\n",
    "\n",
    "\n",
    "# Define input and output interfaces\n",
    "input_text = gr.inputs.Textbox(label=\"Input Text\")\n",
    "output_sentiment = gr.outputs.Textbox(label=\"Sentiment\")\n",
    "\n",
    "# Define function to load CSV data and predict sentiment\n",
    "def predict_csv_sentiment(data):\n",
    "    df = pd.read_csv(data[\"csv\"])\n",
    "    sentiment, _, precision, recall, f1_score, support, accuracy = predict_sentimentcsv(df)\n",
    "\n",
    "    # Assign predicted sentiment to a new column in the DataFrame\n",
    "    df[\"Sentiment\"] = sentiment\n",
    "\n",
    "    # Return the DataFrame as HTML table with summary statistics\n",
    "    summary_stats = f\"Precision: {precision:.2f}, Recall: {recall:.2f}, F1 Score: {f1_score:.2f}, Support: {support:.2f}, Accuracy: {accuracy:.2f}%\"\n",
    "    return f\"{df.to_html()}<br>{summary_stats}\"\n",
    "\n",
    "# Define interfaces for CSV and Text input\n",
    "iface_csv = gr.Interface(fn=predict_csv_sentiment, inputs=csv_input, outputs=\"html\", \n",
    "                         title=\"Swahili Sentiment Analyzer for CSV\", \n",
    "                         description=\"Predict the sentiment and polarity score of Swahili text from a CSV file using an SVM classifier trained on a dataset of Swahili text.\")\n",
    "\n",
    "iface_text = gr.Interface(fn=predict_sentiment, inputs=input_text, outputs=output_sentiment, \n",
    "             title=\"Swahili Sentiment Analyzer for Text\", \n",
    "             description=\"Predict the sentiment of Swahili text using an SVM classifier trained on a dataset of Swahili text.\")\n",
    "\n",
    "\n",
    "# Launch the interfaces\n",
    "iface_text.launch()\n",
    "iface_csv.launch()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
